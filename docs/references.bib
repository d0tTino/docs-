@misc{lacan_sep,
  author       = {Stanford Encyclopedia of Philosophy},
  title        = {Jacques Lacan},
  year         = {2023},
  url          = {https://plato.stanford.edu/entries/lacan/}
}
@article{brown2020gpt3,
  author       = {Brown, Tom B. and others},
  title        = {Language Models are Few-Shot Learners},
  journal      = {arXiv preprint arXiv:2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165}
}

@article{openai2023gpt4,
  author       = {{OpenAI}},
  title        = {GPT-4 Technical Report},
  journal      = {arXiv preprint arXiv:2303.08774},
  year         = {2023},
  url          = {https://arxiv.org/abs/2303.08774}
}

@misc{anthropic2024claude3,
  author       = {{Anthropic}},
  title        = {Introducing the Claude 3 Model Family},
  year         = {2024},
  url          = {https://www.anthropic.com/news/claude-3}
}

@misc{google2024gemini15,
  author       = {{Google DeepMind}},
  title        = {An Early Look at Gemini 1.5},
  year         = {2024},
  url          = {https://blog.google/technology/ai/google-deepmind-gemini-1-5/}
}

@article{liu2023lostmiddle,
  author       = {Liu, Nelson F. and others},
  title        = {Lost in the Middle: How Language Models Use Long Contexts},
  journal      = {arXiv preprint arXiv:2307.03172},
  year         = {2023},
  url          = {https://arxiv.org/abs/2307.03172}
}

@article{hsieh2024ruler,
  author       = {Hsieh, Cheng-Ping and others},
  title        = {RULER: What's the Real Context Size of Your Long-Context Language Models?},
  journal      = {arXiv preprint arXiv:2404.06654},
  year         = {2024},
  url          = {https://arxiv.org/abs/2404.06654}
}

@article{bai2024longbench,
  author       = {Bai, Yushi and others},
  title        = {LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  journal      = {arXiv preprint arXiv:2308.14508},
  year         = {2024},
  url          = {https://arxiv.org/abs/2308.14508}
}

@article{dao2022flashattention,
  author       = {Dao, Tri and others},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  journal      = {arXiv preprint arXiv:2205.14135},
  year         = {2022},
  url          = {https://arxiv.org/abs/2205.14135}
}

@article{kwon2023pagedattention,
  author       = {Kwon, Woosuk and others},
  title        = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  journal      = {arXiv preprint arXiv:2309.06180},
  year         = {2023},
  url          = {https://arxiv.org/abs/2309.06180}
}

@misc{fireworks2023kvcache,
  author       = {{Fireworks AI}},
  title        = {The KV Cache: The Hidden Memory Hog of LLM Inference},
  year         = {2023},
  url          = {https://www.fireworks.ai/blog/kv-cache-the-hidden-memory-hog}
}

@article{rajbhandari2020zero,
  author       = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title        = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  journal      = {arXiv preprint arXiv:1910.02054},
  year         = {2020},
  url          = {https://arxiv.org/abs/1910.02054}
}

@article{press2022trainshort,
  author       = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  title        = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  journal      = {arXiv preprint arXiv:2108.12409},
  year         = {2022},
  url          = {https://arxiv.org/abs/2108.12409}
}

@article{peng2023yarn,
  author       = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  title        = {YaRN: Efficient Context Window Extension of Large Language Models},
  journal      = {arXiv preprint arXiv:2309.00071},
  year         = {2023},
  url          = {https://arxiv.org/abs/2309.00071}
}

@article{ding2024longrope,
  author       = {Ding, Yiran and others},
  title        = {LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  journal      = {arXiv preprint arXiv:2402.13753},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.13753}
}

@article{beltagy2020longformer,
  author       = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {arXiv preprint arXiv:2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150}
}

@article{zaheer2021bigbird,
  author       = {Zaheer, Manzil and others},
  title        = {Big Bird: Transformers for Longer Sequences},
  journal      = {arXiv preprint arXiv:2007.14062},
  year         = {2021},
  url          = {https://arxiv.org/abs/2007.14062}
}

@article{choromanski2022performer,
  author       = {Choromanski, Krzysztof and others},
  title        = {Rethinking Attention with Performers},
  journal      = {arXiv preprint arXiv:2009.14794},
  year         = {2022},
  url          = {https://arxiv.org/abs/2009.14794}
}

@article{rae2019compressive,
  author       = {Rae, Jack W. and others},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  journal      = {arXiv preprint arXiv:1911.05507},
  year         = {2019},
  url          = {https://arxiv.org/abs/1911.05507}
}

@article{dai2019transformerxl,
  author       = {Dai, Zihang and others},
  title        = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  journal      = {arXiv preprint arXiv:1901.02860},
  year         = {2019},
  url          = {https://arxiv.org/abs/1901.02860}
}

@article{xiao2024streamingllm,
  author       = {Xiao, Guangxuan and others},
  title        = {Efficient Streaming Language Models with Attention Sinks},
  journal      = {arXiv preprint arXiv:2309.17453},
  year         = {2024},
  url          = {https://arxiv.org/abs/2309.17453}
}

@article{liu2023ringattention,
  author       = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  title        = {Ring Attention with Blockwise Transformers for Near-Infinite Context},
  journal      = {arXiv preprint arXiv:2310.01889},
  year         = {2023},
  url          = {https://arxiv.org/abs/2310.01889}
}

@article{gu2024mamba,
  author       = {Gu, Albert and Dao, Tri},
  title        = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  journal      = {arXiv preprint arXiv:2312.00752},
  year         = {2024},
  url          = {https://arxiv.org/abs/2312.00752}
}

@article{lewis2021rag,
  author       = {Lewis, Patrick and others},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  journal      = {arXiv preprint arXiv:2005.11401},
  year         = {2021},
  url          = {https://arxiv.org/abs/2005.11401}
}

@article{khandelwal2020knnlm,
  author       = {Khandelwal, Urvashi and others},
  title        = {Generalization through Memorization: Nearest Neighbor Language Models},
  journal      = {arXiv preprint arXiv:1911.00172},
  year         = {2020},
  url          = {https://arxiv.org/abs/1911.00172}
}

@article{borgeaud2022retro,
  author       = {Borgeaud, Sebastian and others},
  title        = {Improving Language Models by Retrieving from Trillions of Tokens},
  journal      = {arXiv preprint arXiv:2112.04426},
  year         = {2022},
  url          = {https://arxiv.org/abs/2112.04426}
}

@misc{meta2024llama31,
  author       = {{Meta AI}},
  title        = {Introducing Llama 3.1},
  year         = {2024},
  url          = {https://ai.meta.com/blog/llama-3-1/}
}
