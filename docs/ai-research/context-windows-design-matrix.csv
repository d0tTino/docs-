Method,Complexity,Typical max effective length,Advantages,Caveats
Position Interpolation / YaRN / StRing,Quadratic,32k–128k,"Simple to implement; preserves in-window quality","Drift or instability beyond trained range; still quadratic"
Longformer / BigBird / Performer,Linear or sparse,100k+,"Scales to long inputs; good for long documents","May lose global information; trained models required"
Transformer-XL / Infini-attention / StreamingLLM,Streaming,100k–1M,"Handles arbitrarily long streams; bounded memory","Compression may lose information; limited joint reasoning"
Ring Attention,Distributed quadratic,1M+,"Exact full attention for extreme lengths","Requires multiple GPUs and high-speed interconnect"
kNN-LM / RETRO / RAG,Retriever-dependent,Unbounded,"Decouples knowledge from window; excels at factual recall","Requires external memory and retriever; alignment complexity"
FlashAttention / PagedAttention / vLLM,Quadratic but IO-optimised,Hardware-dependent,"Makes long context feasible by reducing memory bandwidth; essential for serving","Kernel/serving stack changes required; does not change algorithmic scaling"